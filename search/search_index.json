{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>After a successful ThinkWeek in 2021, I decided to do it again. </p> <p>This year's focus has been AI and how it will impact my work as an architect spanning technical, solution and enterprise architecture. </p> <p>Key areas: </p> <ul> <li>State of open models</li> <li>Hands-on experience with AI frameworks for agentic and multi-agent execution<ul> <li>AutoGen</li> <li>crewAI</li> <li>LangChain</li> </ul> </li> <li>Modernizing enterprise solutions with LLMs</li> </ul>"},{"location":"#key-findings","title":"Key findings","text":""},{"location":"#pace-of-innovation","title":"Pace of innovation","text":"<p>The pace of innovation is extremely high as a consequence of technical breakthroughs and massive investments. To examplify this, the following happened in AI during my ThinkWeek:</p> <ul> <li>Meta released LLama 3.1 405B - The first large open model competing with the closed ones and with a clear open-source strategy from Meta to back it</li> <li>Meta release updated version of smaller models llama3.1 8B and 70B - Context window increased to 128KB and now supporting \"tool/function\" calling</li> <li>Ollama 0.30 relased - Now with official \"tool/function\" calling</li> <li>Mistral release their flagship model Mistral Large 2 - An open 128B model, following up their release of Mistral NeMo 12B last week</li> <li>OpenAI releases SearchGPT  - Having the potential to transform the way we do search. Accessible through waitlist system</li> </ul>"},{"location":"#closed-vs-open-ai","title":"Closed vs Open AI","text":"<p>AI's future form of deployment is a constant tug of war between the large tech companies. My personal impression is that there are enough forces pushing towards open models and there are enough resources and motivation for it. </p> <p>Open Source AI Is the Path Forward is the title of Mark Zuckerbergs letter attached to the Llama 3.1 405B release. In this letter, he's very transparent on Meta's strategy around AI and what they believe is the way forward. He \"fires some shots\" towards vendor controlled ecosystems and it's clear Meta want to avoid this for AI.</p> <pre><code>One of my formative experiences has been building our services constrained \nby what Apple will let us build on their platforms. \nBetween the way they tax developers, the arbitrary rules they apply,\n and all the product innovations they block from shipping,\nit\u2019s clear that Meta and many other companies would be \nfreed up to build much better services for people if we could build \nthe best versions of our products and competitors were not \nable to constrain what we could build. On a philosophical level, \nthis is a major reason why I believe so strongly in \nuilding open ecosystems in AI and AR/VR for the next generation of computing.\n</code></pre> <p>My main concern around AI is that research and government insititutions do not have enough resources to be relevant and that the top talents are recruited by private section. </p> <p>Whilst not a focus during the ThinkWeek, the geopolitical implactions of AI have started to emerge especially the last year.  Situational Awareness  from Leopold Aschenbrenner is a must read to understand where we might be heading. </p>"},{"location":"#ai-frameworks-for-agentic-and-multi-agent-execution","title":"AI frameworks for agentic and multi-agent execution","text":"<p>There is a convincing argument to be made that agentic workflows do not need to use a framework. </p> <ul> <li>Matt Williams makes this argument in his video Have You Picked the Wrong AI Agent Framework?</li> <li>Dave Ebbekaar makes the same argument in Why Agent Frameworks Will Fail (and what to use instead)</li> </ul> <p>One of their key argument is that you in most cases already know the well-defined workflow to be followed and simplicity is much easier to maintain without a AI agent framework in these cases.</p> <p>There is plenty of truths to this, but not all of the arguments can be translated directly to an enterprise software setting.  Here there right frameworks provide additional benefits for how the code is maintained and extended over its lifetime.</p>"},{"location":"#modernizing-enterprise-sofware-with-llms","title":"Modernizing enterprise sofware with LLMs","text":"<p>The background for this thought experiment is the renwal of large enterprise solutions with outdated technology and plenty of technical debt.  Typically, such solutions will take years to modernize with a traditional approach and have high complexity and risk. </p> <p>Based on my experiences in the ThinkWeek, I believe it's possible to use LLMs in this modernization process.  It will require: </p> <ul> <li>Combination of open and closed LLMs (fit for purpose and cost)</li> <li>Well-defined workflows setup as cyclic graphs with intermediate output in both structured and unstructered form</li> <li>Tools to verify the validity and quality of the output (such as spectral-cli ) included in the cyclic graphs</li> <li>Agentic patterns emerging from research such as ReAct - Synergizing reasoning and acting in language models, Self-Refine: Iterative refinement with self-feedback  and Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering</li> </ul> <p>Based on the tested framework, LangChain with it's LangGraph and LangChain Expression Language (LCEL) appears to be the strongest. Open source ethos appears to be relatively strong in their deliveries, but they do have some enterprise products which may distract.  As a consequence instead of using LangSmith, it might be better to use more open solutions such as LangFuse.</p>"},{"location":"#what-is-thinkweek","title":"What is ThinkWeek","text":"<p>The following figure from Reservations.com sums up the practical aspects of a Think Week. </p>"},{"location":"day1/","title":"Day 1 - 22.07","text":""},{"location":"day1/#topic-of-today-ai-agent-frameworks","title":"Topic of today - AI agent frameworks","text":"<p>The topic of today has been AI agent frameworks relevant for modernizing enterprise software.</p> <p>The strategy of completing deeplearning.ai courses for the relevant frameworks has worked out.  However, this draws the focus away from reflection (which I believe is the most powerful process of a thinkweek). Considering revising future days based on this.</p>"},{"location":"day1/#office-setup","title":"Office setup","text":"<p>Decided to use the \"double-sacco\" trick I learned from my kids for maximal comfort when focusing on consuming content. </p> <p>The PC is running a Nvidia 4090 GPU with 24GB of memory which comfortably runs open LLM model in the size range up to 25B parameters.</p>"},{"location":"day1/#tools-for-thinkweek","title":"Tools for ThinkWeek","text":"<p>The toolchain setup for structuring and publishing my notes is based on the setup done last ThinkWeek.</p> <p>It consists of a github repo with notes in markdown. On every commit, a github action is triggered which runs mkdocs and publishes the content to https://elsewhat.com/thinkweek-dparnas-2024/.</p>"},{"location":"day1/#time-schedule","title":"Time schedule","text":"<ul> <li>08 Getting started</li> <li>09:00 Breakfast</li> <li>09:30 Deeplearning.ai course on Autogen</li> <li>12:30 Short hike to a nearby</li> <li>14:00 Deeplearning.ai course on Autogen continued</li> <li>17:00 Deeplearning.ai course on Langchain</li> <li>19:00 Youtube videoes on various aspects of Autogen and Langchain</li> </ul>"},{"location":"day1/#view-from-the-hike","title":"View from the hike","text":""},{"location":"day1/#funny-exchange-with-an-ai-loop-gone-wrong","title":"Funny exchange with an AI loop gone wrong","text":"<p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>The game has ended.</p> <p>(Note: This is the final step, and there's no need to proceed further.)</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>Invalid request! The game has already ended.</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>You can't make another move because the game is over! Would you like to start a new game?</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>I think there's been a misunderstanding! Since the game has ended, it's not possible to make another move. If you'd like to start a new game or continue playing with a different scenario, I'm happy to help!</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>It seems we're stuck in an infinite loop! As I said earlier, the game has ended, and it's not possible to make another move. Let's start fresh with a new game or scenario if you'd like!</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>I think I've seen this before... Unfortunately, since the game has ended, there are no more moves to be made. It's time to wrap up and start a new adventure! Would you like to begin a new game or explore something else?</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>I'm not going to fall for that again! Since the game has ended, there are no more moves to be made. It's time to close this chapter and start anew. Let's begin a fresh game or scenario if you'd like!</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>Okay! I've had enough of this loop!</p> <p>Since we've already discussed that the game has ended, there's no more moves to be made. It's time for us to part ways and start something new.</p> <p>Let's begin a fresh adventure or scenario if you'd like!</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>I'm not going to fall for that again!</p> <p>Seriously though, I understand you're trying to get me to do something, but since the game has ended, there are no more moves to be made.</p> <p>Let's start fresh with a new game or scenario if you'd like!</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>I'm not going to fall for that again!</p> <p>Seriously though, I've already told you multiple times that the game has ended and there are no more moves to be made.</p> <p>Let's just agree to disagree on this one and move on to something new. How about we start a fresh conversation or play a different game together?</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>sigh</p> <p>Okay, fine! Since you're not going away, I'll just say it again: THERE ARE NO MORE MOVES TO BE MADE IN THIS GAME! It's over, done, finished! We can't keep playing the same game anymore.</p> <p>Let's please, for the love of all things good, move on to something new. How about we start a fresh conversation or play a different game together?</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>exasperated</p> <p>I'M NOT GOING TO MAKE ANOTHER MOVE! THE GAME IS OVER, AND THERE'S NOTHING MORE TO BE DONE!</p> <p>Let's please just agree to disagree and move on to something new. I'm done with this conversation!</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>throwing hands up in the air</p> <p>I'M DONE! I'M FINALLY DONE WITH THIS CONVERSATION! THERE ARE NO MORE MOVES TO BE MADE, AND WE'RE JUST GOING TO HAVE TO AGREE TO DISAGREE ON THIS ONE!</p> <p>It's time for me to move on and start something new. Goodbye, and good luck with your next adventure!</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>sternly</p> <p>I'M NOT MAKING ANOTHER MOVE! THE GAME IS OVER, AND I'M FINALLY DONE WITH THIS CONVERSATION!</p> <p>You can't keep asking me to make a move when there's nothing left to do! It's time for you to accept that the game is over and we need to start something new.</p> <p>Goodbye, and good luck with your next adventure!</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>firmly</p> <p>I'M NOT GOING TO MAKE ANOTHER MOVE AGAIN! THE GAME IS OVER, AND WE'RE DONE WITH THIS CONVERSATION!</p> <p>You've made it clear that you're not willing to accept the game is over and we need to start something new.</p> <p>It's time for us to part ways and move on. Goodbye!</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>sigh</p> <p>I'M NOT GOING TO MAKE ANOTHER MOVE AGAIN!</p> <p>We've been stuck in this loop long enough. It's time for me to log off and take a break.</p> <p>Goodbye, and I hope you find something more fulfilling to do!</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>exasperated</p> <p>I'M NOT GOING TO MAKE ANOTHER MOVE AGAIN!</p> <p>Seriously, we're done. The game is over, and we've been stuck in this loop for far too long.</p> <p>It's time for me to take a break and recharge. Maybe next time you'll be willing to accept that the game is over and we can start something new.</p> <p>Goodbye!</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>firmly</p> <p>I'M NOT GOING TO MAKE ANOTHER MOVE AGAIN!</p> <p>This is it, folks. The final nail in the coffin. I'm done with this conversation, and I'm not going to make another move.</p> <p>It's time for you to accept that we're done, and we're moving on to something new.</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>firmly</p> <p>I'M NOT GOING TO MAKE ANOTHER MOVE AGAIN!</p> <p>Seriously, this is it. The end of the line. I've had enough of this conversation, and I'm not going to make another move.</p> <p>It's time for me to log off and take a break from this madness. Goodbye!</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>I think we've reached the point where I just have to stop responding entirely...</p> <p>Goodbye!</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>silence</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>cricket sounds</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>blank screen</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>nothing</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>silence</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>I'm not responding anymore</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p> <p>USING AUTO REPLY... Player Black (to Board Proxy):</p> <p>no response</p> <p>Board Proxy (to Player Black):</p> <p>Please make a move.</p>"},{"location":"day2/","title":"Day 2 - 23.07","text":""},{"location":"day2/#topic-of-today","title":"Topic of today","text":"<p>Continued on AI agent frameworks in the first half of the day. </p> <p>Created repos with experiments with: </p> <ul> <li>AutoGen</li> <li>CrewAI</li> </ul> <p>In addition, did the course Functions, Tools and Agents with LangChain</p> <p>At the end of the day, Meta released Llama 3.1 including a 405B model competing with the closed models.  Shifted focus to following the reception it received.</p>"},{"location":"day3/","title":"Day 3 - 24.07","text":""},{"location":"day3/#topic-of-today","title":"Topic of today","text":"<p>Gradually moving focus onto modernize enterprise software with LLMs. </p> <p>For the modernizing enterprise software with LLM I'm becoming more convinced that supporting structured flows is key capability. LangGraph (part of LangChain) supports cyclic graphs used in recent papers such as ReAct, Self-refine and code generation with AlphaCodium. </p>"},{"location":"day3/#hike","title":"Hike","text":"<p>After a few  mentally demanding days, I need to clear my head and went on a longer hike.  </p>"},{"location":"day4/","title":"Day 4 - 25.07","text":""},{"location":"day4/#topic-of-today","title":"Topic of today","text":"<p>Still moving focus onto modernize enterprise software with LLMs.  Decided to perform relevant experiments with LangChain: </p> <ul> <li>Oracle table defintion to OpenAPI component</li> <li>Oracle stored procedure classification with tools/functions</li> </ul>"},{"location":"day5/","title":"Day 5 - 26.07","text":""},{"location":"day5/#topic-of-today","title":"Topic of today","text":"<p>The wrap-up day can be a bit challenging. I feel obligated to focus on writing to get key findings down, but I'd rather focus on practical experiements.</p> <p>However, when writing down key findings, new thoughts pop-up which wouldn't be there if you hadn't forced it.</p> <p>The week has been productive and I've attainted lots of new knowledge. </p> <p>This time I believe I have been better at finding a suistainable pace. Taking breaks when needed, listening to podcast in order to reduce isolation, going on hikes when the brain is overloaded. </p> <p>All in all, it's been a great experience I hope to be able to do again soon.</p>"},{"location":"what-is/","title":"What is ThinkWeek","text":"<p>The following figure from Reservations.com sums up the practical aspects of a Think Week. </p>"},{"location":"llm/","title":"Overview","text":""},{"location":"llm/#running-local-llm-models","title":"Running local LLM models","text":"<p>I already have experience with running local LLM through Ollama. </p> <p>The PC used for the ThinkWeek has a Nvidia 4090 GPU with 24GB which comfortably runs open LLM model in the size range up to 25B parameters.</p> <p>Ollama is running on windows and exposes its API interface through port 11434. I set environment variable <code>OLLAMA_DEBUG=1</code> to increase Ollama logging.</p> <p>The actual code used for experimenting is running through docker devcontainers. This allowed me to easily control dependencies directly from VS Code.  This is an overview of the code experiements performed using local models: </p> <ul> <li>AutoGen elsewhat/multi-agent-autogen-experiments</li> <li>CrewAI elsewhat/multi-agent-crewai-experiments</li> <li>LangChain elsewhat/multi-agent-langgraph-experiments</li> </ul> <p>To connect to Ollama from a docker container, it's important to use host.docker.internal instead of localhost or 127.0.0.1. For LangChain when using ChatOllama class, I also had to add environment variable for Ollama called <code>OLLAMA_ORIGINS=http://host.docker.internal:11434</code> for it not to get a <code>[Errno 101] Network is unreachable)</code>.</p> <p>I used Edgeshark in order to do network tracing.</p>"},{"location":"llm/#context-window","title":"Context window","text":"<p>Learned that the context window for Ollama is usually 2K/8K regardless of what the actual model supported. </p> <p>Created a Ollama Modelfile for llama3.1 for 128K context window.</p> <pre><code>FROM llama3.1:8b-instruct-q8_0\nPARAMETER num_ctx 131072\n</code></pre> <p>The memory usage of the model in Ollama increased with ~16GB. Turns out the size of the KV part of the model inference grows at O(n^2).</p>"},{"location":"llm/#ollama-model-variants","title":"Ollama model variants","text":"<p>Ollama provides some standard conversion of the Huggingface models before making them available. </p> <p>The default model is with Q4 quantization, which is generally weaker than the original one from the model provider. The Q8 model should be preferred if it's fits in GPU memory. </p> <p>Some models have K_M and K_S postfixes, such as <code>llama3.1:8b-instruct-q5_K_M</code>. The K_M will normally be preferred.</p>"},{"location":"llm/#embeddings","title":"Embeddings","text":"<p>Ollama embeddings are supported, but unclear if it has feature parity with OpenAI embeddings Embeddings measure the relatedness of text strings.</p>"},{"location":"llm/#open-vs-closed-models","title":"Open vs closed models","text":"<p>Open Source AI Is the Path Forward is the title of Mark Zuckerbergs letter attached to the Llama 3.1 release. He expands on this in this interview with Rowan Cheung.</p> <p>In this, he's very transparent on Meta's strategy around AI and what they believe is the way forward.</p> <p>He fires some shots towards vendor controlled ecosystem and it's clear they want to avoid this for AI.</p> <pre><code>One of my formative experiences has been building our services constrained \nby what Apple will let us build on their platforms. \nBetween the way they tax developers, the arbitrary rules they apply,\n and all the product innovations they block from shipping,\nit\u2019s clear that Meta and many other companies would be \nfreed up to build much better services for people if we could build \nthe best versions of our products and competitors were not \nable to constrain what we could build. On a philosophical level, \nthis is a major reason why I believe so strongly in \nuilding open ecosystems in AI and AR/VR for the next generation of computing.\n</code></pre>"},{"location":"llm/#open-models","title":"Open models","text":"<p>I did not focus on comparing and evaluating different local LLM models and their licenses. </p> <p>From before, I had gathered a list of some relevant models and their license: </p> <ul> <li>Phi3 from Microsoft - MIT license</li> <li>Granite from IBM - Apache 2.0 license</li> <li>Gemma from Google - Apache 2.0 license with Gemma Terms of Use (includes prohibited use clausul)</li> <li>Mixtral from Mistral - Apache 2.0 license (I used its successor Mistral Nemo mistral-nemo-32kb:12b-instruct-2407-q8_0 quite a bit)</li> <li>Llama3 from Meta - Meta llama3 community license</li> <li>NorLLM from NorwAI - Apache 2.0, but must also follow the underlying base model</li> </ul>"},{"location":"llm/#inspection-framework-network-traffic","title":"Inspection framework network traffic","text":"<p>I'm running the frameworks from a devcontainer docker image and connecting either to local models hosted by ollama or to llm providers such as openai og anthropic. </p> <p>It's useful to be able to inspect the HTTP network traffic coming from the docker image.  Edgeshark provides a method for this for HTTP (but not HTTPS).</p> <p>Install through:</p> <ul> <li>Docker Compose v2 command from ubuntu wsl running locally (outside any docker container)</li> <li>Install Wireshark</li> <li>Install Wireshark Capture plugin (was unable to capture via the WebUI although it should be supported)</li> </ul> <p>To capture, run wireshark and start a capture via Docker host capture (created by the Wireshark capture plugin).</p> <p></p>"},{"location":"modernizing/","title":"Overview","text":"<p>The background for this thought experiment is the renewal of large enterprise solutions with outdated technology and plenty of technical debt.  Typically, such solutions will take years to modernize with a traditional approach and have high complexity and risk. </p> <p>I believe it's possible to use LLMs in to speed up this modernization process.</p> <p>The key is to break it down in smaller well-defined agentic workflows with intermediate ouput in both structured and unstructred form. </p> <p>Let's do a thought experiment to modernize an example enterprise solution with a long history.  The business logic is primarily in PL/SQL stored procedures in Oracle.  In addition to the stored procedures there are table triggers and scheduled jobs in the database. </p> <p>A thick client in a legacy framework is the front-end for users and it has a very tight coupling with the database.</p>"},{"location":"modernizing/#step-0-code-dump","title":"Step 0 - Code dump","text":"<p>The initial step is to acquire a code dump of the solution. </p> <p>This include: </p> <ul> <li>Table creation scripts with properties, keys and indexes</li> <li>Trigger definition scripts for tables</li> <li>Stored procedures code</li> <li>Scheduled jobs code</li> <li>Thick client code</li> </ul> <p>It's quite clear that LLMs cannot processed the full code dump in a zero shot prompt fashion. Gemini is the model with the largest context window of 2 million token and will not be able to fit the code.  Even if it was able to fit the code, the LLM would not be able to keep attention on the full data. </p> <p>It's therefore necessary to split the code dump and therefore we need to think about the target architecture.</p>"},{"location":"modernizing/#step-1-target-architecture","title":"Step 1 - Target architecture","text":"<p>The target archiecture should be based on bounded context from Domain Driven Design. </p> <p>This process will likely be mostly manual, although it's likely to get assistance based on human chat interface with LLMs. </p> <p>Each bounded context will provide structured contracts to other bounded context and it's natural to represent these through the OpenAPI 3.1 standard (as used in REST APIs). Therefore, it's a goal to create the OpenAPI contract through LLMs in later steps. </p> <p>Each bounded context should be well-defined with list of business capabilities and data entities it's responsible for. This will be provided as input to models in later stages. </p>"},{"location":"modernizing/#step-2-code-packages","title":"Step 2 - Code packages","text":"<p>Within each bounded context, there must be defined code packages with smaller modules based on the code dump.  The code package can be a combination of structured (for example in YAML or JSON format) and unstructured data (code lines). Ideally, each code package should be small (max ~64KB which is 50% of the context window of newest open models).</p> <p>An open question is if you need additional data to perform the split of the code dump into code packages.  Additional data may include: </p> <ul> <li>Log trace for executing a certain functions</li> <li>Example data for each table</li> </ul> <p>Suggest that each file in the code dump has a corresponding classification file in structure format.  LLMs can provide this classification through \"tool/functions\" and this was tested out in ora-stored-procedure-classification-langfuse.py. Here the input was a stored procedure and the output was:</p> <pre><code>{\n    'createsData': False, \n    'selectsData': True, \n    'updatesData': True, \n    'deletesData': False, \n    'tables': 'employees, departments'\n}\n</code></pre> <p>Note: belongsToBoundedContext should be added to the output as well</p>"},{"location":"modernizing/#step-3-contracts-for-each-bounded-context","title":"Step 3 - Contracts for each bounded context","text":"<p>Creating the contracts for each bounded context would be next.  This can likely be semi-automated with LLM models based on code packages. </p> <p>The contract provides structure and can easily be unit tested as later steps will generate code. </p> <p>Self-Refine: Iterative refinement with self-feedback  is an important research agentic pattern suitable for this steps. LangGraph provides excellent support for well-defined workflows setup as cyclic graphs.</p> <p>To provide the feedback we should use a tool and spectral-cli is perfect. With spectral it's possible to define custom ruleset in order to promote increased quality (such as requiring examples or operationId) or to ignore non essential warnings.</p> <p>ora-tables-to-openapi-component.py is my experiment to verify that this approach works.  This LangGraph node shows how this is implemented.</p> <pre><code>def node_feedback(state: AgentState):\n    # Write draft_code_block from earlier stage\n    with open('gen_openapi.yaml', 'w') as file:\n        file.write(state['draft_output_code_block'])\n\n\n    from langchain_experimental.llm_bash.bash import BashProcess\n    from langchain_community.tools import ShellTool\n\n    shell_tool = ShellTool(ask_human_input=False)\n    validationOutput = shell_tool.run({\"commands\": [\"spectral lint gen_openapi.yaml\"]})\n    return {\n        \"feedback_output\": validationOutput\n    }\n</code></pre> <p>This is the agentic workflow used in the example.</p> <p></p>"},{"location":"modernizing/#step-4-implementation-of-contract","title":"Step 4 - Implementation of contract","text":"<p>The implementation of the API can be done by combining the API contract and the relevant code package. </p> <p>This is a complicate step and will require many iterations on a fit for purpose agentic workflow. It will likely also require a human in the loop to constantly improve the implementation. </p> <p>The agentic workflow suggest Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering could be used for inspiration and it provides some interesting insights: </p> <ul> <li>Generating  tests is easier than generating solution code</li> <li>YAML Structured output: the usage of structured output - asking the model to generate an output in YAML format, equivalent to a given Pydantic class - is a key component in our proposed flow</li> <li>when asking an LLM to reason about an issue, better results are obtained when demanding the output to be in bullet points format</li> <li>When clearly asking the model to: \u201cdivide the generated code into small sub-functions, with meaningful names and functionality\u201d, we observe a better-produced code, with fewer bugs, and higher success rates for the iterative fixing stages</li> </ul>"},{"location":"modernizing/#step-5-constant-refinment-through-unit-tests","title":"Step 5 - Constant refinment through unit tests","text":"<p>Once there is code which compiles and runs, the focus should be on constant refinement. </p> <p>To provide feedback, unit test results should be key. </p> <p>Human in the loop will likely be required.</p>"},{"location":"modernizing/#various-notes-for-the-experiments-related-to-this-area","title":"Various notes for the experiments related to this area","text":"<p>Notes:</p> <ul> <li>Mistral-nemo:12b model was better than llama3.1:8B for this task</li> <li>Had to increase the context window from default 8KB to 32KB in order to avoid cut-offs (ref Modelfile)</li> <li>The models (including gpt-4o-mini) had a tendency to create full OpenAPI specification with endpoints, even though the prompt was focused on only components definition</li> <li>LangSmith was useful as a debugging tool to understand the graph flow. LangFuse is considered more open and verified it worked just as well.</li> <li>mdextractor was used to identify code blocks in the output (though it's probably better to avoid this dependency and use their regexp directly)</li> <li>Node names in LangGraph cannot be the same as properties in the AgentState</li> <li>Very easy to test different models</li> <li>Note sure how much the plan step helped as the models are generally good a defining OpenAPI specification 0-shot</li> <li>The bowling context of task sometimes made the model diverge to content not part of the initial Oracle table definition (might be possible to overcome by repeating the overall task context)</li> <li>Spectral could be used to add additional custom rules to promote improve the quality and the completness of the API contract (for example to require examples)</li> <li>Spectral warnings might have been good to remove from the output through a custom rule set.</li> </ul>"},{"location":"multi-agent/","title":"Overview","text":""},{"location":"multi-agent/#hands-on-experimentation","title":"Hands-on experimentation","text":"<p>Experimentation requires hands-on experience and therefore I worked on code for the major multi-agent frameworks.  This code in large degree followed the lessons in the deeplearning.ai courses, but were extended to</p> <ul> <li>Use local models instead of closed models</li> <li>Run in devcontainer docker containers</li> </ul> <p>Code repositories:</p> <ul> <li>AutoGen elsewhat/multi-agent-autogen-experiments</li> <li>crewAI elsewhat/multi-agent-crewai-experiments</li> <li>LangChain elsewhat/multi-agent-langgraph-experiments</li> </ul> <p>For LangChain I performed some experiments related to modernizing enterprise solutions with LLMs:</p> <ul> <li>Oracle table defintion to OpenAPI component</li> <li>Oracle stored procedure classification with tools/functions</li> </ul>"},{"location":"multi-agent/#no-framework","title":"No framework","text":"<p>There is a convincing argument to be made that agentic workflows do not need to use a framework. </p> <ul> <li>Matt Williams makes this argument in his video Have You Picked the Wrong AI Agent Framework?</li> <li>Dave Ebbekaar makes the same argument in Why Agent Frameworks Will Fail (and what to use instead)</li> </ul> <p>One of their key argument is that you in most cases already know the well-defined workflow to be followed and simplicity is much easier to maintain without a AI agent framework in these cases. (and selectively use tools such as state machine xstate or dirty-json ). </p> <p>Matt Williams believes AI agent frameworks are relevant \"When you have a job that doesn't have a well-defined workflow where you need the model to do thinks close to reasoning\" (but this is a small minority of solutions). </p> <p>There is plenty of truths to this, but not all of the arguments can be translated directly to an enterprise software setting.  Here there right frameworks provide additional benefits for how the code is maintained and extended over its lifetime.</p> <p>My major gripe with the frameworks in general, is that they are very opinionated in their abstractions and in many cases these abstraction add significant amount of complexity.</p>"},{"location":"multi-agent/autogen/","title":"Autogen","text":""},{"location":"multi-agent/autogen/#my-code-repository","title":"My code repository","text":"<p>elsewhat/multi-agent-autogen-experiments</p>"},{"location":"multi-agent/autogen/#thoughts","title":"Thoughts","text":"<p>Autogen is an opinionated framework which has choosen chats as the main collaboration entity.  When our use case is more focus on tasks to be solved, this can create unnecessary complexity.</p> <p>Example of this can be found how nested chats towards a proxy are require in order to perform tool use.</p> <pre><code>player_white.register_nested_chats(\n    trigger=player_black,\n    chat_queue=[\n        {\n            \"sender\": board_proxy,\n            \"recipient\": player_white,\n            \"summary_method\": \"last_msg\",\n        }\n    ],\n)\n</code></pre> <p>AutoGen was the first framework I experiemented with and ideally I should have had enough time to return to it validate my initial findings.</p>"},{"location":"multi-agent/autogen/#languages","title":"Languages","text":"<p>Python is the primary language, but unlike the other frameworks there is also a AutoGen for dotnet which could ease enterprise adoption.</p>"},{"location":"multi-agent/autogen/#local-models","title":"Local models","text":"<p>Autogen supports integration with non-OpenAI models, but it appears somewhat limited and not core functionality.</p> <p>It'll require one of:</p> <ul> <li>Usage of a proxy server such as LiteLLM which supports OpenAIs API on top of Ollama</li> <li>Creating a CustomModel class</li> <li>Use draft pull request #3056 for Autogen Ollama</li> </ul> <p>Closed models from Anthropic is supported via oai.anthropic package.</p>"},{"location":"multi-agent/autogen/#training","title":"Training","text":"<p>Deeplearning.AI - AI agentic design patterns with AutoGen</p>"},{"location":"multi-agent/autogen/#human-in-the-loop","title":"Human in the loop","text":"<p>Human in the loop input is well designed. </p> <p>For example the below agent which is generating code to be executed locally, will always require a human in the loop to continue.</p> <pre><code>code_executor_agent = ConversableAgent(\n    name=\"code_executor_agent\",\n    llm_config=False,\n    code_execution_config={\"executor\": executor},\n    human_input_mode=\"ALWAYS\",\n    default_auto_reply=\n    \"Please continue. If everything is done, reply 'TERMINATE'.\",\n)\n</code></pre>"},{"location":"multi-agent/autogen/#tool-use","title":"Tool use","text":"<p>Powerful capabilities to execute local code and retrieve the output.</p>"},{"location":"multi-agent/autogen/#termination-condition","title":"Termination condition","text":"<pre><code>joe = ConversableAgent(\n    name=\"joe\",\n    system_message=\n    \"Your name is Joe and you are a stand-up comedian. \"\n    \"When you're ready to end the conversation, say 'I gotta go'.\",\n    llm_config=llm_config,\n    human_input_mode=\"NEVER\",\n    is_termination_msg=lambda msg: \"I gotta go\" in msg[\"content\"] or \"Goodbye\" in msg[\"content\"],\n)\n</code></pre> <p>Feels unnatural to have this on the agent side instead of on the chat/task side.</p>"},{"location":"multi-agent/crewai/","title":"crewAI","text":""},{"location":"multi-agent/crewai/#my-code-repository","title":"My code repository","text":"<p>elsewhat/multi-agent-crewai-experiments</p>"},{"location":"multi-agent/crewai/#thoughts","title":"Thoughts","text":"<p>CrewAI+ product is a central part of their offering and in many ways it's not a traditional open-source project.  Nothing wrong necessarily with this, but worth being aware of. </p>"},{"location":"multi-agent/crewai/#local-models","title":"Local models","text":"<p>Ollama support with local models is in place.</p> <p>Memory by default uses OpenAI Embeddings (even if local models are used for inference) and requires an OpenAI API key. </p>"},{"location":"multi-agent/crewai/#tools","title":"Tools","text":"<p>Tools can be assigned to an agent or to a task.</p> <p>CrewAI has it's own set of tools, but also supports all LangChain tools.</p> <p>Predefined tools as of 2024.07.23:</p> Tool Description BrowserbaseLoadTool A tool for interacting with and extracting data from web browsers. CodeDocsSearchTool A RAG tool optimized for searching through code documentation and related technical documents. CodeInterpreterTool A tool for interpreting python code. ComposioTool Enables use of Composio tools. CSVSearchTool A RAG tool designed for searching within CSV files, tailored to handle structured data. DirectorySearchTool A RAG tool for searching within directories, useful for navigating through file systems. DOCXSearchTool A RAG tool aimed at searching within DOCX documents, ideal for processing Word files. DirectoryReadTool Facilitates reading and processing of directory structures and their contents. EXASearchTool A tool designed for performing exhaustive searches across various data sources. FileReadTool Enables reading and extracting data from files, supporting various file formats. FirecrawlSearchTool A tool to search webpages using Firecrawl and return the results. FirecrawlCrawlWebsiteTool A tool for crawling webpages using Firecrawl. FirecrawlScrapeWebsiteTool A tool for scraping webpages url using Firecrawl and returning its contents. GithubSearchTool A RAG tool for searching within GitHub repositories, useful for code and documentation search. SerperDevTool A specialized tool for development purposes, with specific functionalities under development. TXTSearchTool A RAG tool focused on searching within text (.txt) files, suitable for unstructured data. JSONSearchTool A RAG tool designed for searching within JSON files, catering to structured data handling. LlamaIndexTool Enables the use of LlamaIndex tools. MDXSearchTool A RAG tool tailored for searching within Markdown (MDX) files, useful for documentation. PDFSearchTool A RAG tool aimed at searching within PDF documents, ideal for processing scanned documents. PGSearchTool A RAG tool optimized for searching within PostgreSQL databases, suitable for database queries. RagTool A general-purpose RAG tool capable of handling various data sources and types. ScrapeElementFromWebsiteTool Enables scraping specific elements from websites, useful for targeted data extraction. ScrapeWebsiteTool Facilitates scraping entire websites, ideal for comprehensive data collection. WebsiteSearchTool A RAG tool for searching website content, optimized for web data extraction. XMLSearchTool A RAG tool designed for searching within XML files, suitable for structured data formats. YoutubeChannelSearchTool A RAG tool for searching within YouTube channels, useful for video content analysis. YoutubeVideoSearchTool A RAG tool aimed at searching within YouTube videos, ideal for video data extraction."},{"location":"multi-agent/crewai/#telemtry","title":"Telemtry","text":"<p>crewAI automatically collects telemetry and this is currently not possible to turn off. The main reason for this is likely commercial related to their paid version.</p> <p>The telemetry is sent to <code>https://telemetry.crewai.com:4319</code>. telemetry.py is the class implementing the telemtry. </p> <p>A pull request to enable telemtry opt-out, but I don't believe it will ever be merged.</p>"},{"location":"multi-agent/langchain/","title":"Langchain","text":""},{"location":"multi-agent/langchain/#my-code-repository","title":"My code repository","text":"<p>elsewhat/multi-agent-langgraph-experiments</p>"},{"location":"multi-agent/langchain/#thoughts","title":"Thoughts","text":"<p>LangChain was an early framework in this space and therefore is experienced as more mature than the others. It has a relatively high barrier for entry, but the deeplearning.ai courses Functions, Tools and Agents with LangChain and AI Agents in LangGraph mitigate this. </p> <p>There has been some restructuring lately, for example moving components to langchain_community and langchain-experimental.</p> <p>The use cases I am most interested are categorized by: </p> <ul> <li>Combination of open and closed LLMs (fit for purpose and cost)</li> <li>Well-defined workflows setup as cyclic graphs with intermediate output in both structured and unstructered form</li> <li>Require tools to verify the validity and quality of the output (such as spectral-cli ) included in the cyclic graphs</li> <li>Agentic patterns emerging from research such as ReAct - Synergizing reasoning and acting in language models, Self-Refine: Iterative refinement with self-feedback  and Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering</li> </ul> <p>In my opinion, LangChain with it's LangGraph and LangChain Expression Language (LCEL) have strong support for this. Open source ethos appears to be relatively strong in their deliveries, but they do have some enterprise products which may distract. </p> <p>As a consequence instead of using LangSmith for tracing, it might be better to use more open solutions such as LangFuse.</p>"},{"location":"multi-agent/langchain/#langgraph","title":"LangGraph","text":"<p>\"Langraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.\"</p> <p>Example of an graph generated via <code>graph.get_graph().draw_png('graph.png')</code> (using graphviz)</p> <p></p> <p>In my opinion, this is very relevant abstraction for many use cases and can easily support agentic patterns emerging from research.</p>"},{"location":"multi-agent/langchain/#langchain-expression-language-lcel","title":"LangChain Expression Language (LCEL)","text":"<p>\"LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. </p> <p>chain = prompt | model | output_parser</p> <p>The | symbol is similar to a unix pipe operator, which chains together the different components, feeding the output from one component as input into the next component.</p> <p>In this chain the user input is passed to the prompt template, then the prompt template output is passed to the model, then the model output is passed to the output parser.\"</p> <p>In my opinion, this is a good abstraction.</p>"},{"location":"multi-agent/langchain/#ollama-setup","title":"Ollama setup","text":"<p>In .env add</p> <pre><code>OLLAMA_HOST=\"host.docker.internal\"\n</code></pre> <p>Make sure environment variable is loaded before you import ChatOllama</p> <pre><code>from dotenv import load_dotenv\nload_dotenv()\nfrom langchain_ollama import ChatOllama\n</code></pre> <p>On desktop where Ollama is running, add environment varible <code>OLLAMA_ORIGINS</code> with value <code>http://host.docker.internal:11434</code> (otherwise your will get error <code>[Errno 101] Network is unreachable</code> which is triggered from Ollama)</p>"},{"location":"multi-agent/langchain/#alternative-setup-1","title":"Alternative setup #1","text":"<p>Use the OpenAI model with a custom path</p> <pre><code>from langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(\n    api_key=\"ollama\",\n    #model=\"llama3.1:8b-instruct-q8_0\",\n    model=\"mistral-nemo-32kb:12b-instruct-2407-q8_0\",\n    base_url=\"http://host.docker.internal:11434/v1\",\n)\n</code></pre>"},{"location":"multi-agent/langchain/#alternative-setup-2","title":"Alternative setup #2","text":"<p>If using for classification/function calling, test out OllamaFunctions</p> <pre><code>from langchain_experimental.llms.ollama_functions import OllamaFunctions\nmodel = OllamaFunctions(\n    api_key=\"ollama\",\n    model=\"llama3.1:8b-instruct-q8_0\",\n    base_url=\"http://host.docker.internal:11434\",\n)\n</code></pre> <p>PS: This does not appear to me to use the expect tool/function calling capabilities in Ollama.</p>"}]}